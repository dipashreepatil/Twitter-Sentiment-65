# -*- coding: utf-8 -*-
"""Copy of Copy of Twitter_frontend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFyE5i1bLnUFCrvnxXCgwWNthUV20E_7
"""

!pip install tweet-preprocessor
!pip install jupyter-dash
!pip install dash-bootstrap-components
!pip install tweepy

import tweepy                   # Python wrapper around Twitter API
from google.colab import drive  # to mount Drive to Colab notebook
import json
import csv
import tweepy as tw
import pandas as pd
import numpy as np

#important libraries for preprocessing using NLTK
import preprocessor as p
import nltk
from nltk import word_tokenize, FreqDist
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 
nltk.download
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.tokenize import TweetTokenizer 
from gensim.parsing.preprocessing import remove_stopwords
from textblob import TextBlob
from wordcloud import WordCloud
from matplotlib import pyplot as plt 
import re



#important libraries for plotly dash
import plotly.express as px
import plotly.graph_objects as go
from jupyter_dash import JupyterDash
import dash_core_components as dcc
import dash_html_components as html
import dash_bootstrap_components as dbc
from dash.dependencies import Input, Output

from google.colab import drive
drive.mount('/content/drive')

class data_generation:
  def get_data(self):
    df = pd.read_csv("/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/Master Data/Master_tweets_data.csv")
    df = df.drop(['id',	'date',	'place',	'retweets_count',	'favorites_count',	'user_name'	,'followers_count','preprocessed_tweets'],axis =1)
    return df

#Text Preprocessing
class data_preprocess:
  
  def preprocess_tweet(self,row):
    text = row['text']
    text = p.clean(text)
    return text


  def stopword_removal(self,row):
    text = row['text']
    text = remove_stopwords(text)
    return text

  def lemmatize_text(self,row):
     lemmatizer = nltk.stem.WordNetLemmatizer()
     w_tokenizer = TweetTokenizer()
     text = row['text']
     lemmatized_words = [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]
     sentence = ' '.join(word for word in lemmatized_words)
     return sentence

  def text_preprocessing(self,df1):
      for i in range(len(df1)):
          txt = df1.loc[i]["text"]
          txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags
          txt=re.sub(r'^[RT]+','',txt)#replace RT-tags
          txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs
          txt=re.sub("[^a-zA-Z]", " ",txt)#replace hashtags
          df1.at[i,"text"]=txt

      df1['text'] = df1['text']
      df1['text'] = df1.apply(self.preprocess_tweet, axis=1)
      df1['text'] = df1.apply(self.stopword_removal, axis=1) 
      df1['text'] = df1['text'].str.lower().str.replace('[^\w\s]',' ').str.replace('\s\s+', ' ')
      df1['text'] = df1.apply(self.lemmatize_text, axis=1)

      return df1

class analysis:
  # Function to get the subjectivity score
  def getSubjectivity(self,text):
      return TextBlob(text).sentiment.subjectivity

  #A function to get the polarity of text
  def getPolarity(self,text):
      return  TextBlob(text).sentiment.polarity

  # Create a function to compute negative (-1), neutral (0) and positive (+1) analysis
  def getAnalysis(self,score):
      if score< 0:
        return 0
      elif score >= 0:
        return 1

  def sentiment_analysis(self,df1):
      # Create two new columns 'Subjectivity' & 'Polarity'
      df1['subjectivity'] = df1['text'].apply(self.getSubjectivity)
      df1['polarity'] = df1['text'].apply(self.getPolarity)
      df1['sentiment'] = df1['polarity'].apply(self.getAnalysis)
     
      return df1

  def plot_wordcloud(self,df1):
      # word cloud visualization
      allWords = ' '.join([twts for twts in df1['text']])
      wordCloud = WordCloud(width=1000, height=500, random_state=21, max_font_size=110).generate(allWords)
      plt.imshow(wordCloud, interpolation="bilinear")
      plt.axis('off')
      plt.show()

class model:
  def senti_classifier(self,df1):
    from sklearn.feature_extraction.text import CountVectorizer
    cv = CountVectorizer()
    X = cv.fit_transform(df1['text']).toarray()
    y = df1['sentiment']

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)
    
    from sklearn.naive_bayes import ( BernoulliNB, ComplementNB, MultinomialNB,)
    
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.neural_network import MLPClassifier
    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis


    classifiers = {
    "BernoulliNB": BernoulliNB(),
    "ComplementNB": ComplementNB(),
    "MultinomialNB": MultinomialNB(),
    "KNeighborsClassifier": KNeighborsClassifier(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "RandomForestClassifier": RandomForestClassifier(),
    "LogisticRegression": LogisticRegression(),
    "MLPClassifier": MLPClassifier(max_iter=1000),
    "AdaBoostClassifier": AdaBoostClassifier(),
    }

    from sklearn.metrics import confusion_matrix, accuracy_score

    score_list = []
    classifier_list = []
    for name, sklearn_classifier in classifiers.items():
        classifier = (sklearn_classifier)
        classifier.fit(X_train, y_train)
        y_pred = classifier.predict(X_test)
        
        cm = confusion_matrix(y_test, y_pred)
        score = accuracy_score(y_test, y_pred)
        classifier_list.append(name)
        score_list.append(score)
        print(name , score)
    
    return classifier_list, score_list

dg = data_generation()
da = analysis()
dp = data_preprocess()
m = model()

dataset = dg.get_data()

dataset = dp.text_preprocessing(dataset)

dataset = da.sentiment_analysis(dataset)
da.plot_wordcloud(dataset)

ans = m.senti_classifier(dataset)

