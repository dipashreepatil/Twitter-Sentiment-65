# -*- coding: utf-8 -*-
"""RNN_basic_GRU_LSTM 2.0

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PhSb7IBIhvrpQeVUJJcm5w0EaASka_h9

## **Load Dataset**
Dataset : 

News headlines dataset which is annotated using MLPClassifier which turns out be be performing well (74% accuracy) in our own sentiment analyzer that we have developed to annotate the text.


*   Datapoints in the subset = 67k
*   Dataset is preprocessed for natural language processing application.
"""

from google.colab import drive #Connect to google drive
import pandas as pd

drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/3 idiots Research/Research Data/67k_preprocessed_tokenized_data.csv")

"""## **Import required libraries**"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import gensim
import os
import numpy as np

"""## **Some Functions for word embeddings**"""

def create_tokens_list(df):
  # text_lines is list of list consisting of tokens of each line
  text_tokens_list = list()
  lines = df['text'].values.tolist() # list of all news
  
  for line in lines:
    tokens = word_tokenize(line)
    text_tokens_list.append(tokens) # lists of lists containing tokens of a news headline

  return text_tokens_list

def create_embedding_index(filepath):
  #Embedding index stores word2vec for each word in key-value pair format
  embedding_index = {}
  f = open(os.path.join('', filepath), encoding = "utf-8")
  for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:])
    embedding_index[word] = coefs
 
  f.close()
  print("Length of Embedding Index : ",len(embedding_index))
  return embedding_index

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

def create_sequences(text_tokens_list):
  #vectorize text samples into 2D tensor
  tokenizer_obj = Tokenizer()
  tokenizer_obj.fit_on_texts (text_tokens_list)
  sequences = tokenizer_obj.texts_to_sequences(text_tokens_list) 

  print("tokenizer_obj: ",tokenizer_obj)
  print("sequences[0]: ",sequences[0])

  word_index = tokenizer_obj.word_index
  print("Found %d unique tokens." % len(word_index))

  return word_index,sequences

def calculate_max_length(df):
  max_length = max([len(s.split()) for s in df['text']]) #max length of text in corpus
  print("max length : ",max_length)
  return max_length

def padding_sequences(df,sequences,max_length):
  text_pad = pad_sequences(sequences, maxlen = max_length)
  sentiment  = df['Label'].values

  print("Shape of text tensor: ", text_pad.shape)
  print("Shape of sentiment tensor: ",sentiment.shape)

  return text_pad, sentiment

#mapping embedding from the loaded word2vec model for each word to the 
#tokenizer_obj.word_index vocabulary and create a matrix with word vectors
def create_embedding_matrix(EMBEDDING_DIM,word_index,embedding_index):

  num_words = len(word_index)+1
  embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))

  for word,i in word_index.items():
    if i>num_words:
      continue
    embedding_vector = embedding_index.get(word)

    if embedding_vector is not None:
    #words not found in embedding index will be all-zeros.
      embedding_matrix[i] = embedding_vector

  print("No of words: ",num_words)
  print("Embedding_matrix: \n",embedding_matrix)
  return embedding_matrix

"""## **Different Word Embeddings**

## 1. Creating word embedding for the selected text corpus

###    Training word embedding


---


* Gensim Implementation of Word2Vec (CBOW)
* Creating embedding matrix for text corpus
"""

#train word2vec model
EMBEDDING_DIM = 300
text_tokens_list = create_tokens_list(df)
model = gensim.models.Word2Vec(sentences =text_tokens_list, size = EMBEDDING_DIM, window=5, workers = 4, min_count = 1)

#vocab size
words = list(model.wv.vocab)
print('Vocabulary size: %d' %len(words))
print(text_tokens_list[:5])

#save word2vec model
filename = 'embedding_word2vec.txt'
model.wv.save_word2vec_format("/content/drive/MyDrive/3 idiots Research/word embeddings/"+filename, binary = False)

filepath = "/content/drive/MyDrive/3 idiots Research/word embeddings/embedding_word2vec.txt"
embedding_index1 = create_embedding_index(filepath)
word_index, sequences = create_sequences(text_tokens_list)

max_length = calculate_max_length(df)
text_pad, sentiment = padding_sequences(df,sequences,max_length)

num_words = len(word_index)+1
embedding_matrix_on_corpus = create_embedding_matrix(EMBEDDING_DIM, word_index, embedding_index1)

"""## 2. Google's Pretrained word2vec"""

#!wget -P /content/drive/MyDrive/3\ idiots\ Research/word\ embeddings/ "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
!pip install gensim
from gensim.models import KeyedVectors

EMBEDDING_FILE = '/content/drive/MyDrive/3 idiots Research/word embeddings/GoogleNews-vectors-negative300.bin.gz' # embedding file saved on google drive
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

MAX_SEQUENCE_LENGTH = 50
EMBEDDING_DIM = 300

text_tokens_list = create_tokens_list(df)
word_index, sequences = create_sequences(text_tokens_list)

num_words = len(word_index)+1
max_length = calculate_max_length(df)
text_pad, sentiment = padding_sequences(df,sequences,max_length)

#mapping embedding from the loaded word2vec model for each word to the 
#tokenizer_obj.word_index vocabulary and create a matrix with word vectors
import numpy as np
num_words = len(word_index)+1
embedding_matrix_google_word2vec = np.zeros((num_words, EMBEDDING_DIM))

for word,i in word_index.items():
  if i>num_words:
    continue
  embedding_vector = word2vec[word] if word in word2vec else np.random.rand(300)
  

  if embedding_vector is not None:
  #words not found in embedding index will be all-zeros.
    embedding_matrix_google_word2vec[i] = embedding_vector
print("Embedding Matrix: \n" , (embedding_matrix_google_word2vec))

"""##3.Stanford's Pretrained Glove"""

#!wget -P /root/input/ -c "https://nlp.stanford.edu/data/glove.6B.zip"
#!unzip "/root/input/glove.6B.zip" -d "/content/drive/MyDrive/3 idiots Research/word embeddings/glove"

filepath = "/content/drive/MyDrive/3 idiots Research/word embeddings/glove/glove.6B.300d.txt"
embedding_index3 = create_embedding_index(filepath)
word_index, sequences = create_sequences(text_tokens_list)

max_length = calculate_max_length(df)
num_words = len(word_index)+1

text_pad, sentiment = padding_sequences(df,sequences,max_length)
embedding_matrix_glove = create_embedding_matrix(EMBEDDING_DIM, word_index, embedding_index3)

"""## **Split Train and Test Data**"""

#split the data into a training  set and validation set
def split_train_test_data(text_pad,sentiment):
  VALIDATION_SPLIT = 0.2
  indices = np.arange(text_pad.shape[0])
  np.random.shuffle(indices)
  text_pad = text_pad[indices]
  sentiment = sentiment[indices]

  num_validation_samples = int(VALIDATION_SPLIT * text_pad.shape[0])

  X_train_pad = text_pad[:-num_validation_samples]
  y_train = sentiment[:-num_validation_samples]
  X_test_pad = text_pad[-num_validation_samples:]
  y_test = sentiment[-num_validation_samples:]


  print("Shape of X_train: ",X_train_pad.shape)
  print("Shape of y_train: ",y_train.shape)

  print("Shape of X_test: ", X_test_pad.shape)
  print("Shape of y_test: ",y_test.shape)

  return X_train_pad,y_train, X_test_pad,y_test

"""# **RNN**

Build the models
"""

#We will be using trained embedding matrix. So set trainable = False  in Embedding layer

from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM,GRU, SimpleRNN, Dropout
from keras.layers.embeddings import Embedding
from keras.initializers import Constant
from sklearn.metrics import roc_curve
import tensorflow as tf

def RNN_Net_simple(num_words,EMBEDDING_DIM,embedding_matrix,max_length):
  recall = tf.keras.metrics.Recall()
  precision = tf.keras.metrics.Precision()
  auc=tf.keras.metrics.AUC()

  # define model
  model = Sequential()
  embedding_layer = Embedding(num_words, 
                            EMBEDDING_DIM,
                            embeddings_initializer = Constant(embedding_matrix),
                            input_length = max_length,
                            trainable = False)

  #add layers
  model.add(embedding_layer)
  model.add(SimpleRNN(units = 128, dropout = 0.2, recurrent_dropout=0.2))
  model.add(Dense(1,activation = 'sigmoid'))
  model.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics=['accuracy',auc,precision,recall])
  model.summary()

  return model

def RNN_Net_deep(num_words,EMBEDDING_DIM,embedding_matrix,max_length):
  recall = tf.keras.metrics.Recall()
  precision = tf.keras.metrics.Precision()
  auc=tf.keras.metrics.AUC()

  # define model
  model = Sequential()
  embedding_layer = Embedding(num_words, 
                            EMBEDDING_DIM,
                            embeddings_initializer = Constant(embedding_matrix),
                            input_length = max_length,
                            trainable = False)

  #add layers
  model.add(embedding_layer)
  model.add(SimpleRNN(units = 128, dropout = 0.2, recurrent_dropout=0.2, return_sequences=True))
  model.add(SimpleRNN(units = 128, dropout = 0.2, recurrent_dropout=0.2, return_sequences=True))
  model.add(SimpleRNN(units = 128, dropout = 0.2, recurrent_dropout=0.2))
 
  model.add(Dense(10,activation='sigmoid'))
  model.add(Dense(1,activation = 'sigmoid'))
  model.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics=['accuracy',auc,precision,recall])
  model.summary()

  return model

def RNN_GRU_Net(num_words,EMBEDDING_DIM,embedding_matrix,max_length):
  recall = tf.keras.metrics.Recall()
  precision = tf.keras.metrics.Precision()
  auc=tf.keras.metrics.AUC()

  # define model
  model = Sequential()
  embedding_layer = Embedding(num_words, 
                            EMBEDDING_DIM,
                            embeddings_initializer = Constant(embedding_matrix),
                            input_length = max_length,
                            trainable = False)

  #add layers
  model.add(embedding_layer)
  model.add(GRU(units = 32, dropout = 0.2, recurrent_dropout=0.2,return_sequences=True))
  model.add(SimpleRNN(units = 128, dropout = 0.2, recurrent_dropout=0.2))
  model.add(Dropout(0.2))
  model.add(Dense(1,activation = 'sigmoid'))
  model.compile(loss = 'binary_crossentropy', optimizer = 'adam',metrics=['accuracy',auc,precision,recall])
  model.summary()

  return model

"""Train the models"""

# checkpoints 
from keras.callbacks import ModelCheckpoint,EarlyStopping

#save checkpoint for every epoch
#filepath="/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5"

#save checkpoint for best accuracy only

# word_embeddings ={"embedding_matrix_on_corpus":embedding_matrix_on_corpus, "embedding_matrix_google_word2vec":embedding_matrix_google_word2vec,"embedding_matrix_glove":embedding_matrix_glove}
models = {"RNN_Net_simple": RNN_Net_simple, "RNN_Net_deep": RNN_Net_deep,"RNN_GRU_Net":RNN_GRU_Net}

X_train_pad,y_train,X_test_pad,y_test = split_train_test_data(text_pad,sentiment)
models_history={}
i=0
embedding = "embedding_marix_google_word2vec"

for model_name, model in models.items():
  filepath="/content/drive/MyDrive/3 idiots Research/Models/RNN/{}-on-{}-weights.best.hdf5".format(embedding, model_name)
  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy' , verbose=1, save_best_only=True, mode='max')
  earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)
  callbacks_list = [checkpoint,earlystopping]


  #change embedding matrix to change the word embedding 
  demo_model = model(num_words, EMBEDDING_DIM,embedding_matrix_google_word2vec,max_length)
  #Fit the model
  history = demo_model.fit(X_train_pad, y_train, validation_data = (X_test_pad, y_test), epochs=10, batch_size=64, callbacks=callbacks_list, verbose=0)
  models_history[model_name]=history

import tensorflow as tf
import datetime

# Clear any logs from previous runs
!rm -rf ./logs/

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# checkpoints 
from keras.callbacks import ModelCheckpoint,EarlyStopping

#save checkpoint for every epoch
#filepath="/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5"

X_train_pad,y_train,X_test_pad,y_test = split_train_test_data(text_pad,sentiment)



#change embedding matrix to change the word embedding 
demo_model = RNN_GRU_Net(num_words, EMBEDDING_DIM,embedding_matrix_google_word2vec,max_length)
#Fit the model
history = demo_model.fit(X_train_pad, y_train, validation_data = (X_test_pad, y_test), epochs=2, batch_size=32, callbacks=[tensorboard_callback])

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

!kill 813

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit



"""# **CNN**

Build the model
"""

from keras.layers.embeddings import Embedding
from keras.initializers import Constant
from keras.callbacks import ModelCheckpoint
from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from keras.models import Model

def ConvNet(num_words, EMBEDDING_DIM, embedding_matrix, max_length,labels_index):
    
    embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            embeddings_initializer = Constant(embedding_matrix),
                            input_length=max_length,
                            trainable=False)
    
    sequence_input = Input(shape=(max_length,), dtype='int32')
    embedded_sequences = embedding_layer(sequence_input)

    convs = []
    filter_sizes = [2,3,4,5,6]

    for filter_size in filter_sizes:
        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)
        l_pool = GlobalMaxPooling1D()(l_conv)
        convs.append(l_pool)


    l_merge = concatenate(convs, axis=1)

    x = Dropout(0.1)(l_merge)  
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)
    preds = Dense(1, activation='sigmoid')(x)

    model = Model(sequence_input, preds)
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['acc'])
    model.summary()
    return model

label_names = ['Pos', 'Neg']

"""Train the model"""

#change embedding matrix to change the word embedding
model = ConvNet(num_words, EMBEDDING_DIM, embedding_matrix,max_length, len(list(label_names)))