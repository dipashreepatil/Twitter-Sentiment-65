# -*- coding: utf-8 -*-
"""My_own_SM_Sentiment_Classifier 1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zSDPadrzZac-PeKiqbVu6OE3kQpOK6fb
"""

# Make data directory if it doesn't exist
!mkdir -p data
!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/investigating-sentiment-analysis/data/sentiment140-subset.csv.zip -P data
!unzip -n -d data data/sentiment140-subset.csv.zip

!pip install sklearn

import pandas as pd

df = pd.read_csv("data/sentiment140-subset.csv", nrows=30000)
df.head()

df

df.shape

df.polarity.value_counts()

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=1000)
vectors = vectorizer.fit_transform(df.text)
words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())
words_df.head()

X = words_df
y = df.polarity

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # # Create and train a logistic regression
# linreg = LinearRegression()
# linreg.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a logistic regression
# logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)
# logreg.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a random forest classifier
# forest = RandomForestClassifier(n_estimators=50)
# forest.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a linear support vector classifier (LinearSVC)
# svc = LinearSVC()
# svc.fit(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Create and train a multinomial naive bayes classifier (MultinomialNB)
# bayes = MultinomialNB()
# bayes.fit(X, y)

# Test on your own data

pd.set_option("display.max_colwidth", 200)

unknown = pd.read_csv("/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/Master Data/Raw_Master_Data - Copy of tweets_data.csv")
unknown.head()

unknown = unknown.dropna(axis = 0, how = 'all')
unknown = unknown.drop_duplicates()

print(vectorizer.get_feature_names())

unknown.columns

unknown = unknown.drop(['Unnamed: 0'],axis = 1)

import numpy as np

# Put it through the vectoriser
# x = v.fit_transform(df['Review'].apply(lambda x: np.str_(x)))
# transform, not fit_transform, because we already learned all our words
unknown_vectors = vectorizer.transform(unknown['text'].apply(lambda unknown_vectors: np.str_(unknown_vectors)))
unknown_words_df = pd.DataFrame(unknown_vectors.toarray(), columns=vectorizer.get_feature_names())
unknown_words_df.head()

unknown_words_df.shape

unknown['pred_logreg'] = logreg.predict(unknown_words_df)

unknown['pred_logreg_prob'] = logreg.predict_proba(unknown_words_df)[:,1]

# Predict using all our models. 

# Logistic Regression predictions + probabilities
unknown['pred_logreg'] = logreg.predict(unknown_words_df)
unknown['pred_logreg_proba'] = logreg.predict_proba(unknown_words_df)[:,1]

# Random forest predictions + probabilities
unknown['pred_forest'] = forest.predict(unknown_words_df)
unknown['pred_forest_proba'] = forest.predict_proba(unknown_words_df)[:,1]

# SVC predictions
unknown['pred_svc'] = svc.predict(unknown_words_df)

# Bayes predictions + probabilities
unknown['pred_bayes'] = bayes.predict(unknown_words_df)
unknown['pred_bayes_proba'] = bayes.predict_proba(unknown_words_df)[:,1]

unknown

unknown.to_csv("/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/Master Data/Multi_label_raw 25k data.csv")

"""Testing model for accuracy which one gives better accuracy"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print("Training logistic regression")
# logreg.fit(X_train, y_train)
# 
# print("Training random forest")
# forest.fit(X_train, y_train)
# 
# print("Training SVC")
# svc.fit(X_train, y_train)
# 
# print("Training Naive Bayes")
# bayes.fit(X_train, y_train)

"""Analysis in CM"""

from sklearn.metrics import confusion_matrix, accuracy_score

y_true = y_test
y_pred = logreg.predict(X_test)

score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names)

y_true = y_test
y_pred = forest.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names)

y_true = y_test
y_pred = svc.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names)

y_true = y_test
y_pred = bayes.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names)

"""Percentage based CM"""

y_true = y_test
y_pred = logreg.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)

y_true = y_test
y_pred = logreg.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)

y_true = y_test
y_pred = forest.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)

y_true = y_test
y_pred = svc.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)

y_true = y_test
y_pred = bayes.predict(X_test)
score = accuracy_score(y_test, y_pred)
print("Accuracy : ",score)
matrix = confusion_matrix(y_true, y_pred)

label_names = pd.Series(['negative', 'positive'])
pd.DataFrame(matrix,
     columns='Predicted ' + label_names,
     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)



"""Classifier wise start to end"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.tree import DecisionTreeClassifier
# 
# 
# dctClassifier = DecisionTreeClassifier()
# dctClassifier.fit(X,y)
# 
# 
# unknown['pred_dct'] = dctClassifier.predict(unknown_words_df)
# unknown['pred_dct_proba'] = dctClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training Decision Tree Classifier")
# dctClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = dctClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred = dctClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print(matrix_df)
# print(matrix_df_per)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import GradientBoostingClassifier
# 
# 
# xgbClassifier = GradientBoostingClassifier()
# xgbClassifier.fit(X,y)
# 
# 
# unknown['pred_xgb'] = xgbClassifier.predict(unknown_words_df)
# unknown['pred_xgb_proba'] = xgbClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training XGBoost Classifier")
# xgbClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = xgbClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred = xgbClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print("Confusion matrix of XGBoost")
# print(matrix_df)
# print("Confusion Matrix Based on percentage ")
# print(matrix_df_per)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import AdaBoostClassifier
# 
# 
# adaClassifier = AdaBoostClassifier()
# adaClassifier.fit(X,y)
# 
# 
# unknown['pred_ada'] = adaClassifier.predict(unknown_words_df)
# unknown['pred_ada_proba'] = adaClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training XGBoost Classifier")
# adaClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = adaClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred = adaClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print("Confusion matrix of AdaBoost Classifier")
# print(matrix_df)
# print("Confusion Matrix Based on percentage ")
# print(matrix_df_per)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.neighbors import KNeighborsClassifier 
# 
# 
# knClassifier = KNeighborsClassifier()
# knClassifier.fit(X,y)
# 
# 
# unknown['pred_kn'] = knClassifier.predict(unknown_words_df)
# unknown['pred_kn_proba'] = knClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training KNeighbors Classifier")
# knClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = knClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred =knClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print("Confusion matrix of KNeighbors Classifier")
# print(matrix_df)
# print("Confusion Matrix Based on percentage ")
# print(matrix_df_per)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.linear_model import Perceptron
# 
# 
# prcClassifier = Perceptron(tol=1e-3, random_state=0)
# prcClassifier.fit(X,y)
# 
# 
# unknown['pred_prc'] = prcClassifier.predict(unknown_words_df)
# unknown['pred_prc_proba'] = prcClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training Perceptron Classifier")
# prcClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = prcClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred =prcClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print("Confusion matrix of Perceptron Classifier")
# print(matrix_df)
# print("Confusion Matrix Based on percentage ")
# print(matrix_df_per)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.linear_model import SGDClassifier
# sgdClassifier = SGDClassifier(max_iter=1000, tol=1e-3)
# sgdClassifier.fit(X,y)
# 
# 
# unknown['pred_sgd'] = sgdClassifier.predict(unknown_words_df)
# unknown['pred_sgd_proba'] = sgdClassifier.predict_proba(unknown_words_df)[:,1]
# 
# print(unknown.head(2))
# 
# print("Training SGD Classifier")
# sgdClassifier.fit(X_train,y_train)
# 
# 
# y_true = y_test
# y_pred = sgdClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names)
# 
# y_true = y_test
# y_pred =sgdClassifier.predict(X_test)
# score = accuracy_score(y_test, y_pred)
# print("Accuracy : ",score)
# matrix = confusion_matrix(y_true, y_pred)
# 
# label_names = pd.Series(['negative', 'positive'])
# matrix_df_per = pd.DataFrame(matrix,
#      columns='Predicted ' + label_names,
#      index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)
# print("Confusion matrix of SGD Classifier")
# print(matrix_df)
# print("Confusion Matrix Based on percentage ")
# print(matrix_df_per)

import math
ans = math.gcd(161,28)
ans

