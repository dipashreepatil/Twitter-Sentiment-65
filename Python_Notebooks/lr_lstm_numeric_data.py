# -*- coding: utf-8 -*-
"""LR_LSTM_numeric data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ju-vJlVYjwr684sETXINwO9XXH1WZZGn

# Linear Regression

```


> It internally performs regression math, and predict the value of a dependent variables, based on independent variables.

Here, independent variables are: Open, High, Low, Close and Volume

Dependent is : close
"""

!pip install yfinance
import yfinance as yf  
data = yf.download('TSLA','2019-01-23','2020-01-23')
print(data.head())

import pandas as pd 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

"""**Removing unwanted Columns from the Data Frame**

"""

data = data[['Open','High','Low','Close','Volume']]
print('\n\nData after deleting  Adj Close : ')
print(data.head(5))

print(data.tail(5))

"""**Split into train and test data**"""

data_X = data.loc[:,data.columns !=  'Close' ]
data_Y = data['Close']

#print(data_X)
#We split the data into a 75–25 ratio for training and testing data
train_X, test_X, train_y,test_y = train_test_split(data_X,data_Y,test_size=0.25)
print('\n\nTraining Set : ')
print('\n \n Train_X contains open, high,low')
print(train_X.head())
print(train_y.head())

"""**Creating the Regressor from sci-kit learn’s Linear Regression Module**"""

#Creating the Regressor
regressor = LinearRegression()
regressor.fit(train_X,train_y)

"""**Make predictions and evaluate the results**"""

#Make Predictions and Evaluate the results
predict_y = regressor.predict(test_X)
print('Prediction Score : ' , regressor.score(test_X,test_y))
error = mean_squared_error(test_y,predict_y)
print('Mean Squared Error : ',error)

# preds = model.predict(x_valid)
# rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
# rms
import numpy as np


predicts = regressor.predict(test_X)
rms=np.sqrt(np.mean(np.power((np.array(test_y)-np.array(predicts)),2)))
rms

"""**Plot the predicted and actual values**"""

#Plot the predicted and the expected values

fig = plt.figure()
ax = plt.axes()
ax.grid()
ax.set(xlabel='Close ($)',ylabel='Open ($)', title='Tesla Stock Prediction using Linear Regression')
ax.plot(test_X['Open'],test_y,'-g')
ax.plot(test_X['Open'],predict_y,'-r')
plt.show()

"""**Using seaborn**"""

!pip install seaborn
import seaborn as sns
sns.scatterplot(x=test_X['Open'],y=test_y)
sns.scatterplot(x=test_X['Open'],y=predict_y)

data = yf.download('TSLA','2020-01-24')
data

"""**Predicting today's closing price**"""

todays_data = {'Open':['114.125999'],   #2020-01-24
        'High':['114.772003'],
        'Low':['110.851997'],
        'Volume':['71768000']
        }
tdata = pd.DataFrame(todays_data, columns = ['Open', 'High','Low','Volume'])

predict_y = regressor.predict(tdata)
print(predict_y)

#Actual val:112.963997
#Predicted val:113.17961419

"""Predicting price long away in future"""

data = yf.download('TSLA','2021-03-17')
data

future_data = {'Open':['656.869995'],             #2021-03-17
        'High':['703.72998'], 
        'Low':['651.01001'],
        'Volume':['39508146']
        }
tdata = pd.DataFrame(future_data, columns = ['Open', 'High','Low','Volume'])

predict_y = regressor.predict(tdata)
print(predict_y)

#Actual val : 701.809998
#Predicted val : 691.11300397

"""# Linear regression inference

Linear regression works acceptable if we want to predict today's price, based on past data, including yesterday's.But for predicting in a long way in the future, the accuracy decreases.However, as we have seen, the direction of the price( here upward for tesla ) can be correctly predicted to some extend.
"""

# Inference concluded from following observation table :
from prettytable import PrettyTable 
myTable = PrettyTable(["Actual val", "Predicted val","Data start end date","Date of prediction"]) 

myTable.add_row(["112.963997", "113.17961419","2019-01-23 to 2020-01-23","2020-01-24"]) 
myTable.add_row(["701.809998", "691.11300397","2019-01-23 to 2020-01-23","2021-03-17"]) 

print(myTable)

"""Prediction results

*   Prediction Score :  0.9988323006859013
*   Mean Squared Error :  0.18447212386871079
*   Root meaned square error :0.514975881139115

# **LSTM**

long short term memory - this RNN technique, have the ability to remember important data  and forget unimportant ones.

Imports and fetching data
"""

from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

!pip install yfinance
import yfinance as yf  
data = yf.download('TSLA','2019-01-23','2020-01-23')
print(data.head())

"""New dataframe with cols date and close"""

import pandas as pd
data = data.sort_index(ascending=True, axis=0)
data.reset_index(inplace=True)
new_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Close'])
for i in range(0,len(data)):
    new_data['Date'][i] = data['Date'][i]
    new_data['Close'][i] = data['Close'][i]
  
#setting index
new_data.index = new_data.Date
new_data.drop('Date', axis=1, inplace=True)

new_data

"""Creating xtrain and ytrain"""

import numpy as np
#creating train and test sets
dataset = new_data.values

train = dataset[0:150,:]
valid = dataset[150:,:]

#converting dataset into x_train and y_train
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

x_train, y_train = [], []
for i in range(60,len(train)):
    x_train.append(scaled_data[i-60:i,0])
    y_train.append(scaled_data[i,0])
x_train, y_train = np.array(x_train), np.array(y_train)

"""Fit into lstm"""

x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

# create and fit the LSTM network
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(units=50))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)

#predicting values, using past 60 from the train data
inputs = new_data[len(new_data) - len(valid) - 60:].values
inputs = inputs.reshape(-1,1)
inputs  = scaler.transform(inputs)

X_test = []
for i in range(60,inputs.shape[0]):
    X_test.append(inputs[i-60:i,0])
X_test = np.array(X_test)

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
closing_price = model.predict(X_test)
closing_price = scaler.inverse_transform(closing_price)

"""RMS"""

rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))
rms

closing_price = closing_price.flatten()
closing_price

"""Plot"""

#for plotting
import matplotlib.pyplot as plt
train = new_data[:150]
valid = new_data[150:]
valid['Predictions'] = closing_price
plt.plot(train['Close'],label='train')
plt.plot(valid[['Close']],label='close')
plt.plot(valid[['Predictions']],label='close predicted')
plt.legend()

"""Inference:

Looking at RMS values and the graph, this model is not good enough for predicting values.
"""