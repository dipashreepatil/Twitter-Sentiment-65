# -*- coding: utf-8 -*-
"""All CLassifiers_Working.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o-N7KMZt9HYuOUDO8FsagLbx6NR7CzNS
"""

import pandas as pd

dataset = pd.read_csv("/content/drive/MyDrive/B-Tech Project 20-21 Group 7/Datasets/Master_tweets_data.csv")
dataset

pat1 = r'@[A-Za-z0-9]+' # this is to remove any text with @....
pat2 = r'https?://[A-Za-z0-9./]+'  # this is to remove the urls
combined_pat = r'|'.join((pat1, pat2)) 
pat3 = r'[^a-zA-Z]' # to remove every other character except a-z & A-Z
combined_pat2 = r'|'.join((combined_pat,pat3)) # we combine pat1, pat2 and pat3 to pass it in the cleaning steps

len(dataset['text'])

import nltk
nltk.download('stopwords')

import re
# import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
cleaned_tweets = []

for i in range(0, len(dataset['text'])) :
    tweets = re.sub(combined_pat2,' ',dataset['text'][i])
    tweets = tweets.lower()
    tweets = tweets.split()
    tweets = [ps.stem(word) for word in tweets if not word in set(stopwords.words('english'))]
    tweets = ' '.join(tweets)
    cleaned_tweets.append(tweets)

cleaned_tweets[10:20]

dataset.columns

dataset['cleaned_tweets'] = cleaned_tweets

!pip install vader

nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
for tweet in cleaned_tweets[:10]:
    print(tweet)
    s = sia.polarity_scores(tweet)
    for k in sorted(s):
        print('{0}: {1}, '.format(k, s[k]), end='')
        print()

'''
def findpolarity(data):
    sid = SentimentIntensityAnalyzer()
    polarity = sid.polarity_scores(data)
    if(polarity['compound'] >= 0.2):  
        sentiment = 1
    if(polarity['compound'] <= -0.2):
        sentiment = -1 
    if(polarity['compound'] < 0.2 and polarity['compound'] >-0.2):
        sentiment = 0     
    return(sentiment)
'''

def findpolarity(data):
  sid = SentimentIntensityAnalyzer()
  polarity = sid.polarity_scores(data)
  if(polarity['compound'] >= 0):
    sentiment = 1
  if(polarity['compound'] < 0):
    sentiment = 0
  
  return (sentiment)

cleaned_tweets[0]

"""correctly labeled as positive also perform manual check"""

findpolarity(cleaned_tweets[0])

sentiment = []
for i in range(0, len(cleaned_tweets)):
    s = findpolarity(cleaned_tweets[i])
    sentiment.append(s)

print(len(sentiment))
print(len(cleaned_tweets))

tweet_sentiment = pd.DataFrame()
tweet_sentiment['cleaned_tweets'] = cleaned_tweets
tweet_sentiment['sentiment'] = sentiment

tweet_sentiment.to_csv('master_tweet_sentiment.csv', index=False)

tweet_sentiment.shape[0]

positive_tweet = []
negative_tweet = []
neutral_tweet = []

for i in range(0, tweet_sentiment.shape[0]):
    if tweet_sentiment['sentiment'][i] == 0:
        neutral_tweet.append(tweet_sentiment['cleaned_tweets'][i])
    elif tweet_sentiment['sentiment'][i] == 1:
        positive_tweet.append(tweet_sentiment['cleaned_tweets'][i])
    elif tweet_sentiment['sentiment'][i] == -1:
        negative_tweet.append(tweet_sentiment['cleaned_tweets'][i])

negative_tweet[:10]
print(len(negative_tweet))

positive_tweet[:10]
print(len(positive_tweet))

neutral_tweet[:10]
print(len(neutral_tweet))

!pip install wordcloud

from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)
import matplotlib.pyplot as plt

def show_wordcloud(data, title = None):
    wordcloud = WordCloud(
        background_color='black',
        stopwords=stopwords,
        max_words=200,
        max_font_size=40, 
        scale=3,
        random_state=1 # chosen at random by flipping a coin; it was heads
    ).generate(str(data))

    fig = plt.figure(1, figsize=(12, 12))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)

    plt.imshow(wordcloud)
    plt.show()

show_wordcloud(positive_tweet)
show_wordcloud(neutral_tweet)
show_wordcloud(negative_tweet)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X = cv.fit_transform(tweet_sentiment['cleaned_tweets']).toarray()
y = tweet_sentiment['sentiment']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)



from sklearn.naive_bayes import (
    BernoulliNB,
    ComplementNB,
    MultinomialNB,
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

classifiers = {
    "BernoulliNB": BernoulliNB(),
    "ComplementNB": ComplementNB(),
    "MultinomialNB": MultinomialNB(),
    "KNeighborsClassifier": KNeighborsClassifier(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "RandomForestClassifier": RandomForestClassifier(),
    "LogisticRegression": LogisticRegression(),
    "MLPClassifier": MLPClassifier(max_iter=1000),
    "AdaBoostClassifier": AdaBoostClassifier(),
}









from sklearn.metrics import confusion_matrix, accuracy_score

# Use 1/4 of the set for training

score_list = []
classifier_list = []
for name, sklearn_classifier in classifiers.items():
    classifier = (sklearn_classifier)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    
    cm = confusion_matrix(y_test, y_pred)
    score = accuracy_score(y_test, y_pred)
    classifier_list.append(name)
    score_list.append(score)
    print(name , score)
    # classifier.train(features[:train_count])
    # accuracy = nltk.classify.accuracy(classifier, features[train_count:])
    # print(F"{accuracy:.2%} - {name}")

import matplotlib.pyplot as plt
fig,ax = plt.subplots()
fig.autofmt_xdate()
ax.bar(classifier_list,score_list)
plt.show()

import matplotlib.pyplot as plt
import numpy as np


np.random.seed(19680801)


plt.rcdefaults()
fig, ax = plt.subplots()


y_pos = np.arange(len(classifier_list))
performance = 3 + 10 * np.random.rand(len(classifier_list))
error = np.random.rand(len(classifier_list))

ax.barh(y_pos, score_list, xerr=error, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(classifier_list)
ax.invert_yaxis()  # labels read top-to-bottom
ax.set_xlabel('Accuracy Scores')
ax.set_title('Classifiers comparison tweets master')

plt.show()

